---
layout: single
title: "ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬"
categories: Coding
tag: [Crawler, Naver news]
toc: true
toc_sticky: true
toc_label: ëª©ì°¨
author_profile: false
---

##  <img src="C:\Users\hjopy\OneDrive\ë°”íƒ• í™”ë©´\Jake\blog\blog\crackernote.github.io\assets\images\ê·¸ë¦¼1.png" alt="ê·¸ë¦¼1" style="zoom:6%;" /> ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬

### ğŸ“œë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§

- ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ > ì¼ë°˜, ì—°ì˜ˆ, ìŠ¤í¬ì¸  ë‰´ìŠ¤ê°€ ê°ê° ë‹¤ë¥´ê²Œ í¬ë¡¤ë§ í›„ ì—‘ì…€íŒŒì¼ì— ì €ì¥

  ```python
  import requests
  from bs4 import BeautifulSoup
  import pyautogui
  from openpyxl import Workbook
  from openpyxl.styles import Alignment
  
  # ì‚¬ìš©ì ì…ë ¥
  keyword = pyautogui.prompt("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
  lastpage = int(pyautogui.prompt("ëª‡ í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§ í• ê¹Œìš”?"))
  pageNum = 1
  
  # ì—‘ì…€ ìƒì„±í•˜ê¸°
  wb = Workbook()
  
  # ì›Œí¬ ì‹œíŠ¸ ìƒì„±í•˜ê¸°
  ws = wb.create_sheet(f"{keyword}")
  
  # ì—´ ë„ˆë¹„ ì¡°ì ˆ
  ws.column_dimensions["A"].width = 60
  ws.column_dimensions["B"].width = 60
  ws.column_dimensions["C"].width = 80
  
  # í–‰ ë²ˆí˜¸
  row = 1
  
  for i in range(1, lastpage * 10, 10):
      print(f"{pageNum}í˜ì´ì§€ í¬ë¡¤ë§ ì¤‘ì…ë‹ˆë‹¤ ================ ")
      response = requests.get(f"https://search.naver.com/search.naver?where=news&sm=tab_jum&query={keyword}&start={i}")
      html = response.text
      soup = BeautifulSoup(html, 'html.parser')
      articles = soup.select("div.info_group") # ë‰´ìŠ¤ ê¸°ì‚¬ div 10ê°œ ì¶”ì¶œ
      for article in articles:
  
          links = article.select("a.info")
  
          if len(links) >= 2: # ë§í¬ê°€ 2ê°œ ì´ìƒì´ë©´
  
              url = links[1].attrs['href'] # ë‘ë²ˆì§¸ ë§í¬ì˜ href ì¶”ì¶œ
  
              # ë‹¤ì‹œ requestë¥¼ ë‚ ë ¤ ì¤€ë‹¤
              response = requests.get(url, headers={'User-Agent' : 'Mozila/5.0'})
              html = response.text
              soup_sub = BeautifulSoup(html, 'html.parser')
  
              # ì—°ì˜ˆë‰´ìŠ¤ ë˜ëŠ” ìŠ¤í¬ì¸ ë‰´ìŠ¤ëŠ” ì‚¬ì´íŠ¸ì˜ ìƒê¹€ìƒˆê°€ ë‹¤ë¥´ë‹¤
              # ì¦‰, ì˜¤ë¥˜ê°€ ë‚  ìˆ˜ ìˆë‹¤.
              if "entertain" in response.url:
                  title = soup_sub.select_one(".end_tit")
                  content = soup_sub.select_one("#articeBody")
  
              elif "sports" in response.url:
                  title = soup_sub.select_one("h4.title")
                  content = soup_sub.select_one("#newsEndContents")
  
                  # ë³¸ë¬¸ ë‚´ìš©ì•ˆì— ë¶ˆí•„ìš”í•œ div ì‚­ì œ
                  divs = content.select("div")
  
                  for div in divs:
                      div.decompose()
  
              else:
                  title = soup_sub.select_one(".media_end_head_headline")
                  content = soup_sub.select_one("#newsct_article")
  
              print("=======ë§í¬======= \n", url)
              print("=======ì œëª©======= \n", title.text.strip())
              print("=======ë³¸ë¬¸======= \n", content.text.strip())
  
              # ì—‘ì…€ì—  ë§í¬, ì œëª©, ë³¸ë¬¸ ì €ì¥
              ws[f'A{row}'] = url
              ws[f'B{row}'] = title.text.strip()
              ws[f'C{row}'] = content.text.strip()
  
              # ìë™ ì¤„ë°”ê¿ˆ
              ws[f'C{row}'].alignment = Alignment(wrap_text=True)
  
              row = row + 1
  
      # ë§ˆì§€ë§‰ í˜ì´ì§€ ì—¬ë¶€ í™•ì¸ (ë‘ë²ˆì§¸ soup ë³€ìˆ˜ ì´ë¦„ ë³€ê²½)       
      isLastPage = soup.select_one('a.btn_next').attrs['aria-disabled']
      if isLastPage == 'true':
          print("ë§ˆì§€ë§‰ í˜ì´ì§€ ì…ë‹ˆë‹¤.")
          break
  
      pageNum = pageNum + 1
  
  # ì—‘ì…€ ë¬¸ì„œ ì €ì¥í•˜ê¸°
  wb.save(f"{keyword}_result.xlsx")
  ```
  
  
  
